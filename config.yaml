# HyPE System Configuration

# Model configuration
model:
  base_model_name: "Qwen/Qwen2.5-3B-Instruct"
  device: "cuda"
  max_length: 2048
  temperature: 0.7
  top_p: 0.9
  
  # Value Model
  value_head_hidden_dim: 4096
  
  # LoRA configuration
  lora_rank: 8
  lora_alpha: 16
  lora_dropout: 0.05
  lora_target_modules:
    - "q_proj"
    - "v_proj"
  
  # Training
  learning_rate: 0.00001
  batch_size: 32
  num_epochs: 3
  warmup_steps: 100

# Principle Memory configuration
principle_memory:
  milvus_host: "localhost"
  milvus_port: 19530
  collection_name: "principles"
  embedding_model: "BAAI/bge-large-en-v1.5"
  embedding_dim: 1024
  
  # Retrieval
  top_k: 5
  semantic_weight: 0.7
  
  # Deduplication
  duplicate_threshold: 0.85
  merge_threshold: 0.75
  
  # Pruning
  min_credit_score: 0.1
  min_application_count: 3
  max_principles: 100000

# H-MCTS configuration
hmcts:
  search_budget: 30  # Reduced from 100 for faster performance
  exploration_constant: 1.414
  max_depth: 5  # Reduced from 10
  num_hypotheses_per_node: 2  # Reduced from 3
  early_stop_threshold: 0.7  # Stop if Q-value exceeds this
  min_iterations: 5  # Minimum iterations before early stopping
  value_cache_size: 100  # Cache size for value predictions

# SR-Adapt configuration
sr_adapt:
  alignment_threshold: 0.6
  correction_steps: 5
  correction_learning_rate: 0.0001
  max_correction_attempts: 3

# DPVD configuration
dpvd:
  principle_weight: 0.1
  discount_factor: 0.99
  replay_buffer_size: 10000
  training_trigger_threshold: 1000
  value_training_epochs: 3

# DPO configuration
dpo:
  beta: 0.1
  learning_rate: 0.00001
  batch_size: 16
  num_epochs: 3

# General settings
seed: 42
log_level: "INFO"
checkpoint_dir: "./checkpoints"
log_dir: "./logs"
